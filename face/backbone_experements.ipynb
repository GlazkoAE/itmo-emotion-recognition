{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backbone_experements.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/MangaBoba/0b65f8e48d9ba3acb44b2573f8de2d6b/backbone_experements.ipynb)"
      ],
      "metadata": {
        "id": "qGcaEkLJ_CHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_api_key = '' #"
      ],
      "metadata": {
        "id": "t3nHtMe7vSC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WRKASbQHmuhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading git repos, libs, datasets and stuff\n",
        "# resnet18 msceleb pretrain weights\n",
        "!gdown --id 1H421M8mosIVt8KsEWQ1UuYMkQS8X1prf\n",
        "\n",
        "# rafdb aligned\n",
        "!unzip /content/drive/MyDrive/rafdb/labels.zip -d /content\n",
        "!unzip /content/drive/MyDrive/rafdb/images.zip -d /content\n",
        "!unzip /content/Image/aligned.zip -d /content/Image\n",
        "\n",
        "# fer2013\n",
        "# !unzip /content/drive/MyDrive/fer2013.zip \n",
        "\n",
        "# original paper implementaions and some some libs for collab\n",
        "!unzip /content/drive/MyDrive/RUL.zip\n",
        "!git clone https://github.com/JDAI-CV/FaceX-Zoo\n",
        "!git clone https://github.com/amirhfarzaneh/dacl\n",
        "!git clone https://github.com/zyh-uaiaaaa/Relative-Uncertainty-Learning\n",
        "!pip install timm\n",
        "!pip install wandb\n",
        "\n",
        "# jit models\n",
        "!unzip /content/drive/MyDrive/jit_models.zip"
      ],
      "metadata": {
        "id": "OkwEVD0SmvLM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Global imports\n",
        "import sys\n",
        "import os\n",
        "import wandb\n",
        "import time\n",
        "import argparse\n",
        "import pprint\n",
        "from tqdm import tqdm\n",
        "from datetime import timedelta\n",
        "import datetime\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from torchvision import models\n",
        "from torchvision import models as torch_models\n",
        "# from torchsampler import ImbalancedDatasetSampler\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "from datetime import timedelta\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.backends.cudnn as cudnn\n",
        "import socket\n",
        "\n",
        "import warnings\n",
        "from torch.utils.data import sampler\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import seaborn as sn\n",
        "from pandas.core.arrays.numeric import T\n",
        "from PIL import Image \n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import math\n",
        "\n"
      ],
      "metadata": {
        "id": "21ucqrwirx-R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Relative uncertainity learning (RUL):  Original paper implementation  \n",
        "\n",
        "#add gaussian noise\n",
        "def add_g(image_array, mean=0.0, var=30):\n",
        "    std = var ** 0.5\n",
        "    image_add = image_array + np.random.normal(mean, std, image_array.shape)\n",
        "    image_add = np.clip(image_add, 0, 255).astype(np.uint8)\n",
        "    return image_add\n",
        "\n",
        "#flip image\n",
        "def filp_image(image_array):\n",
        "    return cv2.flip(image_array, 1)\n",
        "\n",
        "#set random seed to ensure the results can be reproduced,\n",
        "#we simply set the random seed to 0, change the random seed value might get the performance of RUL better,\n",
        "#but we believe that the random seed parameter should not be finetuned\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#use uncertainty value as weights to mixup feature\n",
        "#we find that simply follow the traditional mixup setup\n",
        "# to get mixup pairs can ensure good performance\n",
        "def mixup_data(x, y, att, use_cuda=True):\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "    att1 = att / (att + att[index])\n",
        "    att2 = att[index] / (att + att[index])\n",
        "    mixed_x = att1 * x + att2 * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, att1, att2\n",
        "\n",
        "#add-up loss\n",
        "def mixup_criterion(y_a, y_b):\n",
        "    return lambda criterion, pred:  0.5 *  criterion(pred, y_a) + 0.5 * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n",
        "                               stride = stride, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
        "                               stride = 1, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
        "                             stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "                        \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, n_blocks, channels, output_dim):\n",
        "        super().__init__()\n",
        "                \n",
        "        \n",
        "        self.in_channels = channels[0]\n",
        "            \n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        \n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "        \n",
        "    def get_resnet_layer(self, block=BasicBlock, n_blocks=[2,2,2,2], channels=[64, 128, 256, 512], stride = 1):\n",
        "    \n",
        "        layers = []\n",
        "        \n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        \n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        \n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        \n",
        "        return x, h\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "@author: Jun Wang \n",
        "@date: 20201019\n",
        "@contact: jun21wangustc@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "# based on:\n",
        "# https://github.com/TreB1eN/InsightFace_Pytorch/blob/master/model.py\n",
        "\n",
        "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
        "import torch\n",
        "\n",
        "class Flatten(Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Conv_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Conv_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "        self.prelu = PReLU(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n",
        "\n",
        "class Linear_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Linear_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class Depth_Wise(Module):\n",
        "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
        "        super(Depth_Wise, self).__init__()\n",
        "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
        "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.residual = residual\n",
        "     def forward(self, x):\n",
        "        if self.residual:\n",
        "            short_cut = x\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_dw(x)\n",
        "        x = self.project(x)\n",
        "        if self.residual:\n",
        "            output = short_cut + x\n",
        "        else:\n",
        "            output = x\n",
        "        return output\n",
        "\n",
        "class Residual(Module):\n",
        "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
        "        super(Residual, self).__init__()\n",
        "        modules = []\n",
        "        for _ in range(num_block):\n",
        "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
        "        self.model = Sequential(*modules)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class MobileFaceNet(Module):\n",
        "    def __init__(self, embedding_size, out_h, out_w):\n",
        "        super(MobileFaceNet, self).__init__()\n",
        "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
        "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
        "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
        "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
        "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
        "        #self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
        "        #self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(4,7), stride=(1, 1), padding=(0, 0))\n",
        "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(out_h, out_w), stride=(1, 1), padding=(0, 0))\n",
        "        self.conv_6_flatten = Flatten()\n",
        "        self.linear = Linear(512, embedding_size, bias=False)\n",
        "        self.bn = BatchNorm1d(embedding_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2_dw(out)\n",
        "        out = self.conv_23(out)\n",
        "        out = self.conv_3(out)\n",
        "        out = self.conv_34(out)\n",
        "        out = self.conv_4(out)\n",
        "        out = self.conv_45(out)\n",
        "        out = self.conv_5(out)\n",
        "        out = self.conv_6_sep(out)\n",
        "        out = self.conv_6_dw(out)\n",
        "        out = self.conv_6_flatten(out)\n",
        "        out = self.linear(out)\n",
        "        out = self.bn(out)\n",
        "        return out\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class res18feature(nn.Module):\n",
        "    def __init__(self, args, pretrained=True, num_classes=7, drop_rate=0.4, out_dim=64):\n",
        "        super(res18feature, self).__init__()\n",
        "\n",
        "        #'affectnet_baseline/resnet18_msceleb.pth'\n",
        "        res18 = ResNet(block=BasicBlock, n_blocks=[2, 2, 2, 2], channels=[64, 128, 256, 512], output_dim=1000)\n",
        "        msceleb_model = torch.load(args.pretrained_backbone_path)\n",
        "        state_dict = msceleb_model['state_dict']\n",
        "        res18.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "        self.out_dim = out_dim\n",
        "        self.features = nn.Sequential(*list(res18.children())[:-2])\n",
        "\n",
        "        self.mu = nn.Sequential(\n",
        "            nn.BatchNorm2d(512, eps=2e-5, affine=False),\n",
        "            nn.Dropout(p=self.drop_rate),\n",
        "            Flatten(),\n",
        "            nn.Linear(512 * 7 * 7, self.out_dim),\n",
        "            nn.BatchNorm1d(self.out_dim, eps=2e-5))\n",
        "\n",
        "        self.log_var = nn.Sequential(\n",
        "            nn.BatchNorm2d(512, eps=2e-5, affine=False),\n",
        "            nn.Dropout(p=self.drop_rate),\n",
        "            Flatten(),\n",
        "            nn.Linear(512 * 7 * 7, self.out_dim),\n",
        "            nn.BatchNorm1d(self.out_dim, eps=2e-5))\n",
        "\n",
        "    def forward(self, x, target, phase='train'):\n",
        "\n",
        "        if phase == 'train':\n",
        "            x = self.features(x)\n",
        "            mu = self.mu(x)\n",
        "            logvar = self.log_var(x)\n",
        "\n",
        "            mixed_x, y_a, y_b, att1, att2 = mixup_data(mu, target, logvar.exp().mean(dim=1, keepdim=True), use_cuda=True)\n",
        "            return mixed_x, y_a, y_b, att1, att2\n",
        "        else:\n",
        "            x = self.features(x)\n",
        "            output = self.mu(x)\n",
        "            return output\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hdx0W-gLqijo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RafDB dataset\n",
        "\n",
        "\n",
        "class RafDataset(data.Dataset):\n",
        "    def __init__(self, phase, transform = None):\n",
        "        self.raf_path = ''\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "        df = pd.read_csv(os.path.join(self.raf_path, \n",
        "                                      '/content/EmoLabel/list_patition_label.txt'), \n",
        "                                  sep=' ', header=None,names=['name','label'])\n",
        "\n",
        "        if phase == 'train':\n",
        "            self.data = df[df['name'].str.startswith('train')]\n",
        "        else:\n",
        "            self.data = df[df['name'].str.startswith('test')]\n",
        "\n",
        "        file_names = self.data.loc[:, 'name'].values\n",
        "        self.label = self.data.loc[:, 'label'].values - 1 # 0:Surprise, 1:Fear, 2:Disgust, 3:Happiness, 4:Sadness, 5:Anger, 6:Neutral\n",
        "\n",
        "        _, self.sample_counts = np.unique(self.label, return_counts=True)\n",
        "        # print(f' distribution of {phase} samples: {self.sample_counts}')\n",
        "\n",
        "        self.file_paths = []\n",
        "        for f in file_names:\n",
        "            f = f.split(\".\")[0]\n",
        "            f = f +\"_aligned.jpg\"\n",
        "            path = os.path.join(self.raf_path, 'Image/aligned', f)\n",
        "            self.file_paths.append(path)\n",
        "\n",
        "    def get_labels(self):\n",
        "      return self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        image = np.array(Image.open(path).convert('RGB'))\n",
        "        label = self.label[idx]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label"
      ],
      "metadata": {
        "id": "zlWi1CDF199m",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title customized RUL model\n",
        "class RUL(nn.Module):\n",
        "    def __init__(self, bbone, num_classes=7, drop_rate=0.4, inp_dim = 512, out_dim=64, feature_size=7):\n",
        "        super(RUL, self).__init__()\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "        self.out_dim = out_dim\n",
        "        self.features = bbone\n",
        "        self.feature_size = feature_size\n",
        "\n",
        "        self.mu = nn.Sequential(\n",
        "            nn.BatchNorm2d(inp_dim, eps=2e-5, affine=False),\n",
        "            nn.Dropout(p=self.drop_rate),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(inp_dim * feature_size * feature_size, self.out_dim),\n",
        "            nn.BatchNorm1d(self.out_dim, eps=2e-5))\n",
        "\n",
        "        self.log_var = nn.Sequential(\n",
        "            nn.BatchNorm2d(inp_dim, eps=2e-5, affine=False),\n",
        "            nn.Dropout(p=self.drop_rate),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(inp_dim * feature_size * feature_size, self.out_dim),\n",
        "            nn.BatchNorm1d(self.out_dim, eps=2e-5))\n",
        "\n",
        "    def forward(self, x, target, phase='train'):\n",
        "\n",
        "        if phase == 'train':\n",
        "            x = self.features(x)\n",
        "            if self.feature_size == 1:\n",
        "              x = torch.unsqueeze(torch.unsqueeze(x,2), 3)\n",
        "            mu = self.mu(x)\n",
        "            logvar = self.log_var(x)\n",
        "\n",
        "            mixed_x, y_a, y_b, att1, att2 = mixup_data(mu, target, logvar.exp().mean(dim=1, keepdim=True), use_cuda=True)\n",
        "            return mixed_x, y_a, y_b, att1, att2\n",
        "        else:\n",
        "            x = self.features(x)\n",
        "            output = self.mu(x)\n",
        "            return output\n"
      ],
      "metadata": {
        "id": "7w8NESazsOYH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUL model for jit trace\n",
        "class evalRUL(nn.Module):\n",
        "    def __init__(self, bbone, num_classes=7, drop_rate=0.4, inp_dim = 512, out_dim=64):\n",
        "        super(evalRUL, self).__init__()\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "        self.out_dim = out_dim\n",
        "        self.features = bbone\n",
        "\n",
        "        self.mu = nn.Sequential(\n",
        "            nn.BatchNorm2d(inp_dim, eps=2e-5, affine=False),\n",
        "            nn.Dropout(p=self.drop_rate),\n",
        "            Flatten(),\n",
        "            nn.Linear(inp_dim * 7 * 7, self.out_dim),\n",
        "            nn.BatchNorm1d(self.out_dim, eps=2e-5))\n",
        "\n",
        "        self.log_var = nn.Sequential(\n",
        "            nn.BatchNorm2d(inp_dim, eps=2e-5, affine=False),\n",
        "            nn.Dropout(p=self.drop_rate),\n",
        "            Flatten(),\n",
        "            nn.Linear(inp_dim * 7 * 7, self.out_dim),\n",
        "            nn.BatchNorm1d(self.out_dim, eps=2e-5))\n",
        "\n",
        "    def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            output = self.mu(x)\n",
        "            return output"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m5c3PzeE9Biz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUL train loop\n",
        "\n",
        "def train_RUL(RUL,\n",
        "          sampling=False,\n",
        "          out_features_dim = 64, \n",
        "          epochs = 60, \n",
        "          batch_size = 64, \n",
        "          workers = 2,\n",
        "          train_tf = None,\n",
        "          val_tf = None):\n",
        "\n",
        "    setup_seed(0)\n",
        "    model = RUL\n",
        "    fc = nn.Linear(out_features_dim, 7)\n",
        "\n",
        "    data_transforms = train_tf\n",
        "    data_transforms_val = test_tf\n",
        "\n",
        "    train_dataset = RafDataset(phase='train', transform=data_transforms)\n",
        "    test_dataset = RafDataset(phase='test', transform=data_transforms_val)\n",
        "\n",
        "    if sampling == False:\n",
        "      train_shuffle = True\n",
        "      train_sampler = None\n",
        "    else:\n",
        "      train_shuffle = False\n",
        "      from collections import Counter\n",
        "      train_dataset.label\n",
        "      sample_weights = [0]*len(train_dataset)\n",
        "      class_weights = []\n",
        "      classes = Counter(train_dataset.label).keys()\n",
        "      classes_num_samples = Counter(train_dataset.label).values()\n",
        "      for s in classes_num_samples:\n",
        "            class_weights.append(1/s)\n",
        "\n",
        "      for idx, (data, label) in enumerate(train_dataset):\n",
        "          class_weight = class_weights[label]\n",
        "          sample_weights[idx] = class_weight\n",
        "      train_sampler = WeightedRandomSampler(sample_weights, \n",
        "                                            num_samples=len(sample_weights), \n",
        "                                            replacement=True)\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler=train_sampler,\n",
        "                                               shuffle=train_shuffle,\n",
        "                                               num_workers=workers,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=workers,\n",
        "                                              pin_memory=True)\n",
        "\n",
        "    model.cuda()\n",
        "    fc.cuda()\n",
        "\n",
        "    params = model.parameters()\n",
        "    params2 = fc.parameters()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': params},\n",
        "        {'params': params2, 'lr': 0.01}], lr=0.0001, weight_decay=0.0001,\n",
        "        amsgrad=True)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "\n",
        "\n",
        "    best_acc = 0\n",
        "    best_epoch = 0\n",
        "    for i in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        iter_cnt = 0\n",
        "        correct_sum = 0\n",
        "        model.train()\n",
        "\n",
        "        for batch_i, (imgs, labels) in enumerate(train_loader):\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            mixed_x, y_a, y_b, att1, att2 = model(imgs, labels, phase='train')\n",
        "            outputs = fc(mixed_x)\n",
        "\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss_func = mixup_criterion(y_a, y_b)\n",
        "            loss = loss_func(criterion, outputs)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            iter_cnt += 1\n",
        "            _, predicts = torch.max(outputs, 1)\n",
        "\n",
        "            correct_num = torch.eq(predicts, labels).sum()\n",
        "            correct_sum += correct_num\n",
        "            running_loss += loss\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss = running_loss / iter_cnt\n",
        "\n",
        "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
        "        #wandb.log(acc , loss)\n",
        "        print('Epoch : %d, train_acc : %.4f, train_loss: %.4f' % (i, acc, running_loss))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            iter_cnt = 0\n",
        "            correct_sum = 0\n",
        "            data_num = 0\n",
        "\n",
        "\n",
        "            for batch_i, (imgs, labels) in enumerate(test_loader):\n",
        "                imgs = imgs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                outputs = model(imgs, labels, phase='test')\n",
        "                outputs = fc(outputs)\n",
        "\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "                iter_cnt += 1\n",
        "                _, predicts = torch.max(outputs, 1)\n",
        "\n",
        "                correct_num = torch.eq(predicts, labels).sum()\n",
        "                correct_sum += correct_num\n",
        "\n",
        "                running_loss += loss\n",
        "                data_num += outputs.size(0)\n",
        "\n",
        "            running_loss = running_loss / iter_cnt\n",
        "            test_acc = correct_sum.float() / float(data_num)\n",
        "\n",
        "            if test_acc > best_acc:\n",
        "                best_acc = test_acc\n",
        "                best_epoch = i\n",
        "                if best_acc >= 0.86:\n",
        "                    torch.save({'model_state_dict': model.state_dict(),\n",
        "                                'fc_state_dict': fc.state_dict()},\n",
        "                               \"model_86.pth\")\n",
        "                    print('Model saved.')\n",
        "\n",
        "\n",
        "            print('Epoch : %d, test_acc : %.4f, test_loss: %.4f' % (i, test_acc, running_loss))\n",
        "\n",
        "    print('best acc: ', best_acc, 'best epoch: ', best_epoch)\n"
      ],
      "metadata": {
        "id": "ewYCogRnzE7A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare lightweight backbones"
      ],
      "metadata": {
        "id": "eq1PGcB9vzNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained on MS-Celeb \n",
        "backbone_pathes = {'MobileFaceNet':'/content/drive/MyDrive/celebp/mobilefacenet/Epoch_17.pt',\n",
        "                   'GhostNet' : '/content/drive/MyDrive/celebp/GhostNet/Epoch_17.pt',\n",
        "                   'ReXNetv1' : '',\n",
        "                   'SwinT' : '/content/drive/MyDrive/celebp/SwinT/Epoch_17.pt',\n",
        "                   'LiteCNN29' : '', \n",
        "                   'ResNet18' : '/content/resnet18_msceleb.pth'\n",
        "                   }"
      ],
      "metadata": {
        "id": "QsNlJDhLt-DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('FaceX-Zoo')\n",
        "from backbone.GhostNet import GhostNet\n",
        "from backbone.MobileFaceNets import MobileFaceNet\n",
        "from backbone.LightCNN import LightCNN\n",
        "from backbone.Swin_Transformer import SwinTransformer"
      ],
      "metadata": {
        "id": "-MnQvCwsw3pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backbone_dict(model_path: str, resnet18 = False) -> dict:\n",
        "\n",
        "  if resnet18:\n",
        "    return torch.load('/content/resnet18_msceleb.pth')['state_dict']\n",
        "\n",
        "  d = torch.load(model_path)['state_dict']\n",
        "  d = {key: val for key, val in d.items() if key.startswith('backbone')}\n",
        "  valid_dict = {key.split(\"backbone.\",1)[1]: val for key, val in d.items()}\n",
        "  return valid_dict"
      ],
      "metadata": {
        "id": "-7kmtuzXzGlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "with open(\"/content/FaceX-Zoo/training_mode/backbone_conf.yaml\", \"r\") as stream:\n",
        "    try:\n",
        "        bbones_cfgs = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)"
      ],
      "metadata": {
        "id": "qPCy9mSY0Euy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchsampler"
      ],
      "metadata": {
        "id": "CEvTnL7PQP-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experements with RUL backbones"
      ],
      "metadata": {
        "id": "4HtzZHTnswDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title transforms config\n",
        "train_tf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        # transforms.RandomApply([\n",
        "        # transforms.RandomAdjustSharpness(sharpness_factor=1),\n",
        "        #     ], p=0.3),\n",
        "        # transforms.RandomApply([\n",
        "        #         transforms.RandomRotation(20),\n",
        "        #         transforms.RandomCrop(224, padding=32)\n",
        "        #     ], p=0.3),\n",
        "        # transforms.RandomApply([\n",
        "        #         transforms.ColorJitter(brightness=0.05, contrast=0.05, \n",
        "        #                               saturation=0.05, hue=0.05)\n",
        "        #     ], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(scale=(0.02, 0.25))\n",
        "    ])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "w9QmiB1q6JC1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbone = ResNet(block=BasicBlock, n_blocks=[2, 2, 2, 2],\n",
        "               channels=[64, 128, 256, 512], output_dim=1000)\n",
        "bbone.load_state_dict(get_backbone_dict(backbone_pathes['ResNet18']),strict=False)\n",
        "feature_extractor = nn.Sequential(*list(bbone.children())[:-2])"
      ],
      "metadata": {
        "id": "1u0jOOghtJMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbone = GhostNet(**bbones_cfgs['GhostNet'])\n",
        "bbone.load_state_dict(get_backbone_dict(backbone_pathes['GhostNet']))\n",
        "feature_extractor = nn.Sequential(*list(bbone.children())[:-1])"
      ],
      "metadata": {
        "id": "sN4XmBfOHhqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbone = MobileFaceNet(*bbones_cfgs['MobileFaceNet'].values())\n",
        "bbone.load_state_dict(get_backbone_dict(backbone_pathes['MobileFaceNet']))\n",
        "feature_extractor = nn.Sequential(*list(bbone.children())[:-4])"
      ],
      "metadata": {
        "id": "zgoxLrn5aTCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use RUL() to train model, use evalRUL() for jit trace "
      ],
      "metadata": {
        "id": "QCJbE9pvHnX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OriginalRUL = evalRUL(feature_extractor)"
      ],
      "metadata": {
        "id": "Z9YBsEDyvgAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MobileRUL = evalRUL(feature_extractor,drop_rate=0.3, #0.2 \n",
        "                out_dim=64)"
      ],
      "metadata": {
        "id": "gLLzFEJs3ShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_feature_size = 64\n",
        "GhostRUL = evalRUL(bbone=feature_extractor, inp_dim=960, out_dim=inner_feature_size)\n"
      ],
      "metadata": {
        "id": "HyqdJOtNKMJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.login(key=wandb_api_key)\n",
        "# run = wandb.init(settings=wandb.Settings(start_method=\"thread\"),\n",
        "#                     entity='mangaboba', project='FER')\n",
        "# run.name = 'teslLog'\n",
        "\n",
        "train_RUL(OriginalRUL,train_tf=train_tf, val_tf=test_tf, \n",
        "          out_features_dim=64)\n",
        "\n",
        "#wandb.finish()"
      ],
      "metadata": {
        "id": "ZfYUV8irvKNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load mobileRUL\n",
        "def load_rul_model(model, pth):\n",
        "    sd = torch.load(pth)\n",
        "    fc = torch.nn.Linear(64, 7)\n",
        "    fc.load_state_dict(sd['fc_state_dict'])\n",
        "    # d = {key: val for key, val in d.items() if key.startswith('features')}\n",
        "    # valid_dict = {key.split(\"features.\",1)[1]: val for key, val in d.items()}\n",
        "    model.load_state_dict(sd['model_state_dict'])\n",
        "    return torch.nn.Sequential(model, fc)\n"
      ],
      "metadata": {
        "id": "erGFd-HL6IfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileRUL = load_rul_model(MobileRUL, '/content/drive/MyDrive/resulet_models/mobileRul_8716_4112.pth')\n",
        "ResnetRUL = load_rul_model(OriginalRUL, '/content/drive/MyDrive/resulet_models/rul_res18_0.888.pth')"
      ],
      "metadata": {
        "id": "YNY6bDeoE1nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/drive/MyDrive/DAN.zip -d .\n",
        "# !unzip DAN.zip"
      ],
      "metadata": {
        "id": "kkqY0_bXAC0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DAN model: https://github.com/yaoing/DAN\n",
        "\n",
        "class DAN(nn.Module):\n",
        "    def __init__(self, bbone, num_class=7,num_head=4, pretrained=True):\n",
        "        super(DAN, self).__init__()\n",
        "        \n",
        "        self.features = bbone\n",
        "        self.num_head = num_head\n",
        "        for i in range(num_head):\n",
        "            setattr(self,\"cat_head%d\" %i, CrossAttentionHead())\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(512, num_class)\n",
        "        self.bn = nn.BatchNorm1d(num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        heads = []\n",
        "        for i in range(self.num_head):\n",
        "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
        "        \n",
        "        heads = torch.stack(heads).permute([1,0,2])\n",
        "        if heads.size(1)>1:\n",
        "            heads = F.log_softmax(heads,dim=1)\n",
        "            \n",
        "        out = self.fc(heads.sum(dim=1))\n",
        "        out = self.bn(out)\n",
        "   \n",
        "        return out, x, heads\n",
        "\n",
        "class CrossAttentionHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = SpatialAttention()\n",
        "        self.ca = ChannelAttention()\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "    def forward(self, x):\n",
        "        sa = self.sa(x)\n",
        "        ca = self.ca(sa)\n",
        "\n",
        "        return ca\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.conv_3x3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3,padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.conv_1x3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=(1,3),padding=(0,1)),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.conv_3x1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=(3,1),padding=(1,0)),\n",
        "            nn.BatchNorm2d(512),\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1x1(x)\n",
        "        y = self.relu(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n",
        "        y = y.sum(dim=1,keepdim=True) \n",
        "        out = x*y\n",
        "        \n",
        "        return out \n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(512, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 512),\n",
        "            nn.Sigmoid()    \n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, sa):\n",
        "        sa = self.gap(sa)\n",
        "        sa = sa.view(sa.size(0),-1)\n",
        "        y = self.attention(sa)\n",
        "        out = sa * y\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "f5Hhw0S3AZym",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DAN train loop\n",
        "class AffinityLoss(nn.Module):\n",
        "    def __init__(self, device, num_class=8, feat_dim=512):\n",
        "        super(AffinityLoss, self).__init__()\n",
        "        self.num_class = num_class\n",
        "        self.feat_dim = feat_dim\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.device = device\n",
        "\n",
        "        self.centers = nn.Parameter(torch.randn(self.num_class, self.feat_dim).to(device))\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.gap(x).view(x.size(0), -1)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_class) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_class, batch_size).t()\n",
        "        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
        "\n",
        "        classes = torch.arange(self.num_class).long().to(self.device)\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_class)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_class))\n",
        "\n",
        "        dist = distmat * mask.float()\n",
        "        dist = dist / self.centers.var(dim=0).sum()\n",
        "\n",
        "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
        "\n",
        "        return loss\n",
        "\n",
        "class PartitionLoss(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(PartitionLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        num_head = x.size(1)\n",
        "\n",
        "        if num_head > 1:\n",
        "            var = x.var(dim=1).mean()\n",
        "            ## add eps to avoid empty var case\n",
        "            loss = torch.log(1+num_head/(var+1e-6))\n",
        "        else:\n",
        "            loss = 0\n",
        "            \n",
        "        return loss\n",
        "\n",
        "def trainin_DAN(DANmodel, batch_size = 256, \n",
        "                 workers = 2, lr = 0.1, epochs = 40):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.enabled = True\n",
        "\n",
        "    model = DANmodel\n",
        "    model.to(device)\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.RandomCrop(224, padding=32)\n",
        "            ], p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(scale=(0.02,0.25)),\n",
        "        ])\n",
        "\n",
        "    # data_transforms = transforms.Compose([\n",
        "    #     transforms.Resize((224, 224)),\n",
        "    #     transforms.RandomHorizontalFlip(),\n",
        "    #     transforms.RandomApply([\n",
        "    #             transforms.RandomRotation(20),\n",
        "    #             transforms.RandomCrop(224, padding=32)\n",
        "    #         ], p=0.3),\n",
        "    #     transforms.RandomApply([\n",
        "    #             transforms.ColorJitter(brightness=0.05, contrast=0.05, \n",
        "    #                                   saturation=0.05, hue=0.025)\n",
        "    #         ], p=0.3),\n",
        "        \n",
        "    #     transforms.ToTensor(),\n",
        "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "    #                              std=[0.229, 0.224, 0.225]),\n",
        "    #     transforms.RandomErasing(scale=(0.02,0.25)),\n",
        "    #     ])\n",
        "    \n",
        "    train_dataset = RafDataset(phase = 'train', transform = data_transforms)    \n",
        "    \n",
        "    print('Whole train set size:', train_dataset.__len__())\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                              # sampler=ImbalancedDatasetSampler(train_dataset),\n",
        "                                               shuffle = True,  \n",
        "                                               batch_size = batch_size,\n",
        "                                               num_workers = workers,\n",
        "                                               pin_memory = True)\n",
        "\n",
        "    data_transforms_val = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])])   \n",
        "\n",
        "    val_dataset = RafDataset(phase = 'test', transform = data_transforms_val)   \n",
        "\n",
        "    print('Validation set size:', val_dataset.__len__())\n",
        "    \n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                               batch_size = batch_size,\n",
        "                                               num_workers = workers,\n",
        "                                               shuffle = False,  \n",
        "                                               pin_memory = True)\n",
        "\n",
        "    criterion_cls = torch.nn.CrossEntropyLoss()\n",
        "    criterion_af = AffinityLoss(device)\n",
        "    criterion_pt = PartitionLoss()\n",
        "\n",
        "    params = list(model.parameters()) + list(criterion_af.parameters())\n",
        "    optimizer = torch.optim.SGD(params,lr=lr, weight_decay = 1e-4, momentum=0.9)\n",
        "    # optimizer = torch.optim.AdamW(params,lr=lr, \n",
        "                                  # weight_decay = 1e-4, amsgrad=True)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        running_loss = 0.0\n",
        "        correct_sum = 0\n",
        "        iter_cnt = 0\n",
        "        model.train()\n",
        "\n",
        "        for (imgs, targets) in train_loader:\n",
        "            iter_cnt += 1\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            out,feat,heads = model(imgs)\n",
        "\n",
        "            loss = criterion_cls(out,targets) + 1* criterion_af(feat,targets) + 1*criterion_pt(heads)  #89.3 89.4\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss\n",
        "            _, predicts = torch.max(out, 1)\n",
        "            correct_num = torch.eq(predicts, targets).sum()\n",
        "            correct_sum += correct_num\n",
        "\n",
        "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
        "        running_loss = running_loss/iter_cnt\n",
        "        tqdm.write('[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f' % (epoch, acc, running_loss,optimizer.param_groups[0]['lr']))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.0\n",
        "            iter_cnt = 0\n",
        "            bingo_cnt = 0\n",
        "            sample_cnt = 0\n",
        "            \n",
        "            ## for calculating balanced accuracy\n",
        "            y_true = []\n",
        "            y_pred = []\n",
        "\n",
        "            model.eval()\n",
        "            for (imgs, targets) in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                \n",
        "                out,feat,heads = model(imgs)\n",
        "                loss = criterion_cls(out,targets) + criterion_af(feat,targets) + criterion_pt(heads)\n",
        "\n",
        "                running_loss += loss\n",
        "                iter_cnt+=1\n",
        "                _, predicts = torch.max(out, 1)\n",
        "                correct_num  = torch.eq(predicts,targets)\n",
        "                bingo_cnt += correct_num.sum().cpu()\n",
        "                sample_cnt += out.size(0)\n",
        "                \n",
        "                y_true.append(targets.cpu().numpy())\n",
        "                y_pred.append(predicts.cpu().numpy())\n",
        "        \n",
        "            running_loss = running_loss/iter_cnt   \n",
        "            scheduler.step()\n",
        "\n",
        "            acc = bingo_cnt.float()/float(sample_cnt)\n",
        "            acc = np.around(acc.numpy(),4)\n",
        "            best_acc = max(acc,best_acc)\n",
        "\n",
        "            y_true = np.concatenate(y_true)\n",
        "            y_pred = np.concatenate(y_pred)\n",
        "            balanced_acc = np.around(balanced_accuracy_score(y_true, y_pred),4)\n",
        "\n",
        "            tqdm.write(\"[Epoch %d] Validation accuracy:%.4f. bacc:%.4f. Loss:%.3f\" % (epoch, acc, balanced_acc, running_loss))\n",
        "            tqdm.write(\"best_acc:\" + str(best_acc))\n",
        "\n",
        "            if acc > 0.89 and acc == best_acc:\n",
        "                torch.save({'iter': epoch,\n",
        "                            'model_state_dict': model.state_dict(),\n",
        "                             'optimizer_state_dict': optimizer.state_dict(),},\n",
        "                            os.path.join('checkpoints', \"rafdb_epoch\"+str(epoch)+\"_acc\"+str(acc)+\"_bacc\"+str(balanced_acc)+\".pth\"))\n",
        "                tqdm.write('Model saved.')\n"
      ],
      "metadata": {
        "id": "crI7x8gWhlKk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbone = torch_models.resnet18()\n",
        "bbone.load_state_dict(get_backbone_dict(backbone_pathes['ResNet18'],\n",
        "                                        resnet18 = True), strict=True)\n",
        "feature_extractor = nn.Sequential(*list(bbone.children())[:-2])\n",
        "resnet18DAN = DAN(bbone = feature_extractor)"
      ],
      "metadata": {
        "id": "DblBn-sgp1dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbone = MobileFaceNet(*bbones_cfgs['MobileFaceNet'].values())\n",
        "bbone.load_state_dict(get_backbone_dict(backbone_pathes['MobileFaceNet']))\n",
        "feature_extractor = nn.Sequential(*list(bbone.children())[:-4])\n",
        "resnet18DAN = DAN(bbone = feature_extractor)"
      ],
      "metadata": {
        "id": "qrmGfNqHO3Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainin_DAN(resnet18DAN, batch_size=128)"
      ],
      "metadata": {
        "id": "scLBqtUUza62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DACL experements\n",
        "https://github.com/amirhfarzaneh/dacl"
      ],
      "metadata": {
        "id": "l2269KNGgTeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DACL imports\n",
        "from dacl.loss import SparseCenterLoss\n",
        "from dacl.utils import Logger, AverageMeter, accuracy, calc_metrics, RandomFiveCrop\n",
        "sys.path.append('dacl')\n",
        "#torchvision.models.utils -> torch.hub\n",
        "from dacl.models.resnet import resnet18"
      ],
      "metadata": {
        "id": "MXrlb90vWMTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DACL modified train loop for for any model https://github.com/amirhfarzaneh/dacl/blob/f1d5aad84650831f70ad82e41919f8d092ce0411/main.py#L311 \n",
        "\n",
        "def train_DACL(cfg, feat_size=512, train_set = None, val_set = None):\n",
        "\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        cudnn.benchmark = True\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    if cfg['deterministic']:\n",
        "        random.seed(cfg['seed'])\n",
        "        torch.manual_seed(cfg['seed'])\n",
        "        cudnn.deterministic = True\n",
        "        cudnn.benchmark = False\n",
        "\n",
        "    # Loading RAF-DB\n",
        "    # -----------------\n",
        "    print('[>] Loading dataset '.ljust(64, '-'))\n",
        "\n",
        "    # validation set\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=cfg['batch_size'], shuffle=True,\n",
        "        num_workers=cfg['workers'], pin_memory=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                               batch_size=cfg['batch_size'],\n",
        "                                              #  sampler=train_sampler,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=cfg['workers'],\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(val_set, batch_size=cfg['batch_size'],\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=cfg['workers'],\n",
        "                                              pin_memory=True)\n",
        "\n",
        "    print('[*] Loaded dataset!')\n",
        "\n",
        "    # Create Model\n",
        "    # ------------\n",
        "    print('[>] Model '.ljust(64, '-'))\n",
        "    if cfg['model'] == 'resnet18':\n",
        "        feat_size = 512\n",
        "        if not cfg['pretrained'] == '':\n",
        "            model = resnet18(pretrained=cfg['pretrained'])\n",
        "            model.fc = nn.Linear(feat_size, 7)\n",
        "        else:\n",
        "            print('[!] model is trained from scratch!')\n",
        "            model = resnet18(num_classes=7, pretrained=cfg['pretrained'])\n",
        "    else:\n",
        "        model = cfg['model']\n",
        "    model = torch.nn.DataParallel(model).to(device)\n",
        "    print('[*] Model initialized!')\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    # ----------------------------------------------\n",
        "    criterion = {\n",
        "        'softmax': nn.CrossEntropyLoss().to(device),\n",
        "        'center': SparseCenterLoss(7, feat_size).to(device)\n",
        "    }\n",
        "    optimizer = {\n",
        "        'softmax': torch.optim.SGD(model.parameters(), cfg['lr'],\n",
        "                                   momentum=cfg['momentum'],\n",
        "                                   weight_decay=cfg['weight_decay']),\n",
        "        'center': torch.optim.SGD(criterion['center'].parameters(), cfg['alpha'])\n",
        "    }\n",
        "    # lr scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer['softmax'], step_size=20, gamma=0.1)\n",
        "\n",
        "    # training and evaluation\n",
        "    # -----------------------\n",
        "    global best_valid\n",
        "    best_valid = dict.fromkeys(['acc', 'rec', 'f1', 'aucpr', 'aucroc'], 0.0)\n",
        "\n",
        "    print('[>] Begin Training '.ljust(64, '-'))\n",
        "    for epoch in range(1, cfg['epochs'] + 1):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, criterion, optimizer, epoch, cfg)\n",
        "        # validate for one epoch\n",
        "        validate(val_loader, model, criterion, epoch, cfg)\n",
        "\n",
        "        # progress\n",
        "        end = time.time()\n",
        "        progress = (\n",
        "            f'[*] epoch time = {end - start:.2f}s | '\n",
        "            f'lr = {optimizer[\"softmax\"].param_groups[0][\"lr\"]}\\n'\n",
        "        )\n",
        "        print(progress)\n",
        "\n",
        "        # lr step\n",
        "        scheduler.step()\n",
        "\n",
        "    # best valid info\n",
        "    # ---------------\n",
        "    print('[>] Best Valid '.ljust(64, '-'))\n",
        "    stat = (\n",
        "        f'[+] acc={best_valid[\"acc\"]:.4f}\\n'\n",
        "        f'[+] rec={best_valid[\"rec\"]:.4f}\\n'\n",
        "        f'[+] f1={best_valid[\"f1\"]:.4f}\\n'\n",
        "        f'[+] aucpr={best_valid[\"aucpr\"]:.4f}\\n'\n",
        "        f'[+] aucroc={best_valid[\"aucroc\"]:.4f}'\n",
        "    )\n",
        "    print(stat)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, cfg):\n",
        "    losses = {\n",
        "        'softmax': AverageMeter(),\n",
        "        'center': AverageMeter(),\n",
        "        'total': AverageMeter()\n",
        "    }\n",
        "    accs = AverageMeter()\n",
        "    y_pred, y_true, y_scores = [], [], []\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=int(len(train_loader.dataset) / cfg['batch_size'])) as pbar:\n",
        "        for i, (images, target) in enumerate(train_loader):\n",
        "\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            feat, output, A = model(images)\n",
        "            l_softmax = criterion['softmax'](output, target)\n",
        "            l_center = criterion['center'](feat, A, target)\n",
        "            l_total = l_softmax + cfg['lamb'] * l_center\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc, pred = accuracy(output, target)\n",
        "            losses['softmax'].update(l_softmax.item(), images.size(0))\n",
        "            losses['center'].update(l_center.item(), images.size(0))\n",
        "            losses['total'].update(l_total.item(), images.size(0))\n",
        "            accs.update(acc.item(), images.size(0))\n",
        "\n",
        "            # collect for metrics\n",
        "            y_pred.append(pred)\n",
        "            y_true.append(target)\n",
        "            y_scores.append(output.data)\n",
        "\n",
        "            # compute grads + opt step\n",
        "            optimizer['softmax'].zero_grad()\n",
        "            optimizer['center'].zero_grad()\n",
        "            l_total.backward()\n",
        "            optimizer['softmax'].step()\n",
        "            optimizer['center'].step()\n",
        "\n",
        "            # progressbar\n",
        "            pbar.set_description(f'TRAINING [{epoch:03d}/{cfg[\"epochs\"]}]')\n",
        "            pbar.set_postfix({'L': losses[\"total\"].avg,\n",
        "                              'Ls': losses[\"softmax\"].avg,\n",
        "                              'Lsc': losses[\"center\"].avg,\n",
        "                              'acc': accs.avg})\n",
        "            pbar.update(1)\n",
        "\n",
        "    metrics = calc_metrics(y_pred, y_true, y_scores)\n",
        "    progress = (\n",
        "        f'[-] TRAIN [{epoch:03d}/{cfg[\"epochs\"]}] | '\n",
        "        f'L={losses[\"total\"].avg:.4f} | '\n",
        "        f'Ls={losses[\"softmax\"].avg:.4f} | '\n",
        "        f'Lsc={losses[\"center\"].avg:.4f} | '\n",
        "        f'acc={accs.avg:.4f} | '\n",
        "        f'rec={metrics[\"rec\"]:.4f} | '\n",
        "        f'f1={metrics[\"f1\"]:.4f} | '\n",
        "        f'aucpr={metrics[\"aucpr\"]:.4f} | '\n",
        "        f'aucroc={metrics[\"aucroc\"]:.4f}'\n",
        "    )\n",
        "    print(progress)\n",
        "    write_log(losses, accs.avg, metrics, epoch, tag='train')\n",
        "    wandb.log({\"tL\": losses[\"total\"].avg})\n",
        "    wandb.log({\"tLs\": losses[\"softmax\"].avg})\n",
        "    wandb.log({\"tLsc\": losses[\"center\"].avg})\n",
        "    wandb.log({\"tacc\": accs.avg})\n",
        "    wandb.log({\"trec\": metrics[\"rec\"]})\n",
        "    wandb.log({\"tf1\": metrics[\"f1\"]})\n",
        "\n",
        "\n",
        "\n",
        "def validate(valid_loader, model, criterion, epoch, cfg):\n",
        "    losses = {\n",
        "        'softmax': AverageMeter(),\n",
        "        'center': AverageMeter(),\n",
        "        'total': AverageMeter()\n",
        "    }\n",
        "    accs = AverageMeter()\n",
        "    y_pred, y_true, y_scores = [], [], []\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with tqdm(total=int(len(valid_loader.dataset) / cfg['batch_size'])) as pbar:\n",
        "        with torch.no_grad():\n",
        "            for i, (images, target) in enumerate(valid_loader):\n",
        "\n",
        "                images = images.to(device)\n",
        "                target = target.to(device)\n",
        "                # compute output\n",
        "                feat, output, A = model(images)\n",
        "                l_softmax = criterion['softmax'](output, target)\n",
        "                l_center = criterion['center'](feat, A, target)\n",
        "                l_total = l_softmax + cfg['lamb'] * l_center\n",
        "\n",
        "                # measure accuracy and record loss\n",
        "                acc, pred = accuracy(output, target)\n",
        "                losses['softmax'].update(l_softmax.item(), images.size(0))\n",
        "                losses['center'].update(l_center.item(), images.size(0))\n",
        "                losses['total'].update(l_total.item(), images.size(0))\n",
        "                accs.update(acc.item(), images.size(0))\n",
        "\n",
        "                # collect for metrics\n",
        "                y_pred.append(pred)\n",
        "                y_true.append(target)\n",
        "                y_scores.append(output.data)\n",
        "\n",
        "                # progressbar\n",
        "                pbar.set_description(f'VALIDATING [{epoch:03d}/{cfg[\"epochs\"]}]')\n",
        "                pbar.update(1)\n",
        "\n",
        "    metrics = calc_metrics(y_pred, y_true, y_scores)\n",
        "    progress = (\n",
        "        f'[-] VALID [{epoch:03d}/{cfg[\"epochs\"]}] | '\n",
        "        f'L={losses[\"total\"].avg:.4f} | '\n",
        "        f'Ls={losses[\"softmax\"].avg:.4f} | '\n",
        "        f'Lsc={losses[\"center\"].avg:.4f} | '\n",
        "        f'acc={accs.avg:.4f} | '\n",
        "        f'rec={metrics[\"rec\"]:.4f} | '\n",
        "        f'f1={metrics[\"f1\"]:.4f} | '\n",
        "        f'aucpr={metrics[\"aucpr\"]:.4f} | '\n",
        "        f'aucroc={metrics[\"aucroc\"]:.4f}'\n",
        "    )\n",
        "    print(progress)\n",
        "\n",
        "    # save model checkpoints for best valid\n",
        "    if accs.avg > best_valid['acc']:\n",
        "        save_checkpoint(epoch, model, cfg, tag='best_valid_acc.pth')\n",
        "        # wandb.sklearn.plot_confusion_matrix(y_true, y_pred, \n",
        "          # ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger', 'Neutral'])\n",
        "    if metrics['rec'] > best_valid['rec']:\n",
        "        save_checkpoint(epoch, model, cfg, tag='best_valid_rec.pth')\n",
        "\n",
        "    best_valid['acc'] = max(best_valid['acc'], accs.avg)\n",
        "    best_valid['rec'] = max(best_valid['rec'], metrics['rec'])\n",
        "    best_valid['f1'] = max(best_valid['f1'], metrics['f1'])\n",
        "    best_valid['aucpr'] = max(best_valid['aucpr'], metrics['aucpr'])\n",
        "    best_valid['aucroc'] = max(best_valid['aucroc'], metrics['aucroc'])\n",
        "    write_log(losses, accs.avg, metrics, epoch, tag='valid')\n",
        "    wandb.log({\"vL\": losses[\"total\"].avg})\n",
        "    wandb.log({\"vLs\": losses[\"softmax\"].avg})\n",
        "    wandb.log({\"vLsc\": losses[\"center\"].avg})\n",
        "    wandb.log({\"vacc\": accs.avg})\n",
        "    wandb.log({\"vrec\": metrics[\"rec\"]})\n",
        "    wandb.log({\"vf1\": metrics[\"f1\"]})\n",
        "\n",
        "\n",
        "\n",
        "def write_log(losses, acc, metrics, e, tag='set'):\n",
        "    # tensorboard\n",
        "    writer.add_scalar(f'L_softmax/{tag}', losses['softmax'].avg, e)\n",
        "    writer.add_scalar(f'L_center/{tag}', losses['center'].avg, e)\n",
        "    writer.add_scalar(f'L_total/{tag}', losses['total'].avg, e)\n",
        "    writer.add_scalar(f'acc/{tag}', acc, e)\n",
        "    writer.add_scalar(f'rec/{tag}', metrics['rec'], e)\n",
        "    writer.add_scalar(f'f1/{tag}', metrics['f1'], e)\n",
        "    writer.add_scalar(f'aucpr/{tag}', metrics['aucpr'], e)\n",
        "    writer.add_scalar(f'aucroc/{tag}', metrics['aucroc'], e)\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, cfg, tag):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "    }, os.path.join(cfg['save_path'], tag))"
      ],
      "metadata": {
        "id": "3ThzRsUdVT8U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DACL experement configuration. Use cfg['model'] = model to set you model (out_features=7). Don't forget to fix input resizing!\n",
        "\n",
        "cfg = {\n",
        "            # model\n",
        "            'model' : 'resnet18', #_your_model_here_ or 'resnet18' fro original\n",
        "            # global config\n",
        "            'log_dir': \"logs\",\n",
        "            'save_path' : '',\n",
        "            'deterministic': False,\n",
        "            'seed': 0,\n",
        "            'workers': 4,\n",
        "            # training config\n",
        "            'arch': 'resnet18',\n",
        "            'lr': 0.01,\n",
        "            'momentum': 0.9,\n",
        "            'weight_decay': 0.0005,\n",
        "            'batch_size': 64,\n",
        "            'epochs': 60,\n",
        "            'pretrained': \"msceleb\",\n",
        "            # centerloss config\n",
        "            'alpha': 0.5,\n",
        "            'lamb': 0.01\n",
        "        }\n",
        "\n",
        "current_time = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "logname = current_time + '_' + socket.gethostname() + \\\n",
        "      (\n",
        "            f'_a-centerloss'\n",
        "            f'_ar={cfg[\"arch\"]}'\n",
        "            f'_pt={cfg[\"pretrained\"]}'\n",
        "            f'_bs={cfg[\"batch_size\"]}'\n",
        "            f'_lr={cfg[\"lr\"]}'\n",
        "            f'_wd={cfg[\"weight_decay\"]}'\n",
        "            f'_alpha={cfg[\"alpha\"]}'\n",
        "            f'_lamb={cfg[\"lamb\"]}'\n",
        "        )\n",
        "cfg['save_path'] = os.path.join(cfg['log_dir'], logname)\n",
        " \n",
        "  # Create a new directory because it does not exist \n",
        "os.makedirs(cfg['save_path'])\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.5752, 0.4495, 0.4012],\n",
        "                                     std=[0.2086, 0.1911, 0.1827]) ## rafdb\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "train_set = RafDataset(phase='train', \n",
        "              transform = transforms.Compose([\n",
        "                          transforms.ToPILImage(),\n",
        "                          transforms.Resize(256),\n",
        "                          RandomFiveCrop(224),\n",
        "                          transforms.RandomHorizontalFlip(),\n",
        "                          transforms.ToTensor(),\n",
        "                          normalize,\n",
        "                      ]))\n",
        "\n",
        "val_set = RafDataset(phase='val', \n",
        "              transform=transforms.Compose([\n",
        "                        transforms.ToPILImage(),\n",
        "                        transforms.Resize(256),\n",
        "                        transforms.CenterCrop(224),\n",
        "                        transforms.ToTensor(),\n",
        "                        normalize,\n",
        "                    ]))\n",
        "\n",
        "global writer\n",
        "writer = SummaryWriter(cfg['log_dir'])\n",
        "sys.stdout = Logger(os.path.join(cfg['log_dir'], 'log.log'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "i-PoEViOWqFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run ResNet18 train"
      ],
      "metadata": {
        "id": "3CF5xiMZlN4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run original implementation\n",
        "# !python dacl/main.py \n",
        "\n",
        "# or\n",
        "cfg['model'] = 'resnet18' # set as default"
      ],
      "metadata": {
        "id": "sK9rAcLblKbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from dacl.models import resnet18\n",
        "# a = resnet18(pretrained='msceleb').cpu().eval()\n",
        "# a.forward(torch.rand((1,3,224,224)))"
      ],
      "metadata": {
        "id": "C6s72KwYr9EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DACL MobileFaceNet implementation \n",
        "\n",
        "class MobileFaceNet_DACL(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(MobileFaceNet_DACL, self).__init__()\n",
        "        self.bbone = MobileFaceNet(*bbones_cfgs['MobileFaceNet'].values())\n",
        "        self.bbone.load_state_dict(get_backbone_dict(backbone_pathes['MobileFaceNet']))\n",
        "        self.feature_extractor = nn.Sequential(*list(self.bbone.children())[:-4])\n",
        "        self.nb_head = 512\n",
        "        # cfg['model'] = \n",
        "        # nn.Sequential(bbone, torch.nn.Linear(feat_size, 7))#####\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 3584),\n",
        "            nn.BatchNorm1d(3584),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(3584, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.attention_heads = nn.Linear(64, 2 * self.nb_head)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.feature_extractor(x)\n",
        "\n",
        "        x_flat = torch.flatten(x, 1)\n",
        "        E = self.attention(x_flat)\n",
        "        A = self.attention_heads(E).reshape(-1, 512, 2).softmax(dim=-1)[:, :, 1]\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        f = torch.flatten(x, 1)\n",
        "        out = self.fc(f)\n",
        "\n",
        "        return f, out, A\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AN4a_rJbp2Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileFaceNet model for DACL train"
      ],
      "metadata": {
        "id": "LCOoLyALlDbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg['model'] = MobileFaceNet_DACL()"
      ],
      "metadata": {
        "id": "oVwp_Iztj5dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import pprint\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datetime import timedelta\n",
        "\n",
        "from torchvision.transforms.transforms import ToPILImage\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "wandb.login(key=wandb_api_key)\n",
        "run = wandb.init(settings=wandb.Settings(start_method=\"thread\"),\n",
        "                    entity='mangaboba', project='FER')\n",
        "run.name = 'OriginalModel'\n",
        "# -----------------\n",
        "start = time.time()\n",
        "train_DACL(cfg, 512, train_set, val_set)\n",
        "end = time.time()\n",
        "# -----------------\n",
        "run.finish()\n",
        "\n",
        "print('\\n[*] Fini! '.ljust(64, '-'))\n",
        "print(f'[!] total time = {timedelta(seconds=end - start)}s')\n",
        "sys.stdout.flush()"
      ],
      "metadata": {
        "id": "EsIHv1uAaJ2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load DACL original model weights example\n",
        "testmodel = resnet18(pretrained=cfg['pretrained'])\n",
        "testmodel.fc = nn.Linear(512, 7)\n",
        "d = torch.load('/content/logs/Jun21_07-41-22_9f2bbcb9e53b_a-centerloss_ar=resnet18_pt=msceleb_bs=64_lr=0.01_wd=0.0005_alpha=0.5_lamb=0.01/best_valid_acc.pth')['model_state_dict']\n",
        "d = {key: val for key, val in d.items() if key.startswith('module')}\n",
        "valid_dict = {key.split(\"module.\",1)[1]: val for key, val in d.items()}\n",
        "testmodel.load_state_dict(valid_dict)\n",
        "originalDACL = testmodel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAcdeLvDXwvU",
        "outputId": "c91620ee-7f6d-4f21-a7e7-c5ce6332d5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make jit models"
      ],
      "metadata": {
        "id": "de0EpW45ImjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "# model = torchvision.models.resnet34(pretrained = True)\n",
        "# model.eval()\n",
        "def make_jit_models(model, file_prefix, inp_size):\n",
        "    sample = torch.rand(1, 3, inp_size, inp_size)\n",
        "    model.to('cuda:0')\n",
        "    model.eval()\n",
        "    scripted_model = torch.jit.trace(model, sample.to('cuda:0'))\n",
        "    scripted_model.save(file_prefix+'_cuda.pth')\n",
        "    model.to('cpu')\n",
        "    scripted_model = torch.jit.trace(model, sample)\n",
        "    scripted_model.save(file_prefix+'_cpu.pth')"
      ],
      "metadata": {
        "id": "LMtlkjxbgq6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_jit_models(model = cfg['model'], file_prefix = 'MobibeDACL', inp_size = 112)"
      ],
      "metadata": {
        "id": "xwOMQ2Mthhep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_jit_models(model = originalDACL, file_prefix='originalDACL', inp_size=224)"
      ],
      "metadata": {
        "id": "zluDNv9zDK9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models Validation"
      ],
      "metadata": {
        "id": "MDZ9lWCsHGa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Validation util for jit_models \n",
        "class GrayscaleExpander:\n",
        "    \"\"\"\n",
        "    Transformation eval with grayscale images\n",
        "    Duplicates channel 3 times\n",
        "    Gets: (1, N, N) grayscale\n",
        "    Returns: (3, N, N) grayscale\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, image: torch.tensor) -> torch.tensor:\n",
        "        return torch.cat([image, image, image], dim=0)\n",
        "\n",
        "class validator:\n",
        "\n",
        "    def __init__(self, batch_size = 64):\n",
        "        self.batch_size = batch_size\n",
        "        self.device = 'cuda:0'\n",
        "        if not torch.cuda.is_available():\n",
        "            raise RuntimeError('Cuda not available.')\n",
        "        self.class_names = ['Surprise', \n",
        "                            'Fear', \n",
        "                            'Disgust', \n",
        "                            'Happiness', \n",
        "                            'Sadness', \n",
        "                            'Anger', \n",
        "                            'Neutral']\n",
        "    \n",
        "    def computeTime(self, model, inp_size, device='cuda'):\n",
        "        inputs = torch.randn(1, 3, inp_size, inp_size)\n",
        "        if device == 'cuda':\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        i = 0\n",
        "        time_spent = []\n",
        "        while i < 300:\n",
        "            start_time = time.time()\n",
        "            with torch.no_grad():\n",
        "                _ = model(inputs)\n",
        "\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()  # wait for cuda to finish (cuda is asynchronous!)\n",
        "            if i != 0:\n",
        "                time_spent.append(time.time() - start_time)\n",
        "            i += 1\n",
        "        # print('Avg execution time (ms): {:.3f}'.format(np.mean(time_spent)))\n",
        "        return np.mean(time_spent)\n",
        "    \n",
        "    def plot_cf_matrix(self, y_true, y_pred, filename = \"conf_mtx\"):\n",
        "        cfm = sklearn.metrics.confusion_matrix(torch.cat(y_true).cpu().numpy(),\n",
        "                                               torch.cat(y_pred).cpu().numpy(),\n",
        "                                               normalize = 'all')\n",
        "        df_cfm = pd.DataFrame(cfm, index = self.class_names, \n",
        "                              columns = self.class_names)\n",
        "        \n",
        "        df_cfmn = df_cfm.astype('float') / df_cfm.sum(axis=1)[:, np.newaxis]\n",
        "        fig, ax = plt.subplots(figsize=(10,10))\n",
        "        \n",
        "        fig = plt.figure(figsize = (15,15))\n",
        "        \n",
        "        cfm_plot = sn.heatmap(df_cfmn, annot=True, fmt='.2f', \n",
        "                              xticklabels=self.class_names,\n",
        "                              yticklabels=self.class_names)\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        cfm_plot.figure.savefig(filename+\".png\")\n",
        "\n",
        "    def make_dataloaders(self, inp_size, model_class):\n",
        "        if model_class == 'DACL':\n",
        "            transform_raf=transforms.Compose([\n",
        "                          transforms.ToPILImage(),\n",
        "                          transforms.Resize(int(inp_size*(256/224))),\n",
        "                          transforms.CenterCrop(inp_size),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "                      ])\n",
        "            # transform_fer = transforms.Compose([\n",
        "            #                 transforms.Resize(int(inp_size*(256/224))),\n",
        "            #                 transforms.Resize(inp_size),\n",
        "            #                 transforms.ToTensor(),\n",
        "            #                 GrayscaleExpander()]\n",
        "            #                 )\n",
        "            \n",
        "        elif model_class == 'RUL':\n",
        "            transform_raf = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize((inp_size,inp_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])])\n",
        "        \n",
        "        dataset_1 = RafDataset(phase = 'test', transform=transform_raf)\n",
        "        data_loader_1 = torch.utils.data.DataLoader(  \n",
        "                dataset_1,\n",
        "                batch_size=self.batch_size, shuffle=True)\n",
        "        \n",
        "        \n",
        "        return [data_loader_1]\n",
        "\n",
        "    def validate(self, model_pth, model_class, inp_size):\n",
        "        dataloaders = self.make_dataloaders(inp_size, model_class)\n",
        "        for dl in dataloaders:\n",
        "          if model_class == 'DACL':\n",
        "              self.validate_DACL(dl, model_class, model_pth, inp_size)\n",
        "          elif model_class == 'RUL':\n",
        "              self.validate_RUL(dl, model_class, model_pth, inp_size)\n",
        "\n",
        "\n",
        "    def validate_DACL(self, dl, model_class, model_pth, inp_size):\n",
        "        device = 'cuda:0'\n",
        "        model = torch.jit.load(model_pth)\n",
        "        accs = AverageMeter()\n",
        "        y_pred, y_true, y_scores = [], [], []\n",
        "\n",
        "        # switch to evaluate mode\n",
        "        model.eval()\n",
        "        losses = {\n",
        "              'softmax': AverageMeter(),\n",
        "              'center': AverageMeter(),\n",
        "              'total': AverageMeter()\n",
        "              }\n",
        "        criterion = {\n",
        "            'softmax': nn.CrossEntropyLoss().to(device),\n",
        "            'center': SparseCenterLoss(7, 512).to(device)\n",
        "            }\n",
        "        with tqdm(total=int(len(dl.dataset)/self.batch_size)) as pbar:\n",
        "            with torch.no_grad():\n",
        "                for i, (images, target) in enumerate(dl):\n",
        "\n",
        "                    images = images.to(device)\n",
        "                    target = target.to(device)\n",
        "\n",
        "                    # compute output\n",
        "                    feat, output, A = model(images.to('cuda:0'))\n",
        "                    l_softmax = criterion['softmax'](output, target)\n",
        "                    l_center = criterion['center'](feat, A, target)\n",
        "                    l_total = l_softmax + (0.01 * l_center)\n",
        "                    # measure accuracy and record loss\n",
        "                    acc, pred = accuracy(output, target)\n",
        "                    losses['softmax'].update(l_softmax.item(), images.size(0))\n",
        "                    losses['center'].update(l_center.item(), images.size(0))\n",
        "                    losses['total'].update(l_total.item(), images.size(0))\n",
        "                    accs.update(acc.item(), images.size(0))\n",
        "                    # collect for metrics\n",
        "                    y_pred.append(pred)\n",
        "                    y_true.append(target)\n",
        "                    y_scores.append(output.data)\n",
        "\n",
        "        metrics = calc_metrics(y_pred, y_true, y_scores)\n",
        "        avg_exec_time = self.computeTime(model, inp_size)\n",
        "        progress = (\n",
        "            f'{model_class}-{model_pth}: | '\n",
        "            f'L={losses[\"total\"].avg:.4f} | '\n",
        "            f'Ls={losses[\"softmax\"].avg:.4f} | '\n",
        "            f'Lsc={losses[\"center\"].avg:.4f} | '\n",
        "            f'acc={accs.avg:.4f} | '\n",
        "            f'rec={metrics[\"rec\"]:.4f} | '\n",
        "            f'f1={metrics[\"f1\"]:.4f} | '\n",
        "            f'Evaluation time={avg_exec_time:4f} |'\n",
        "        )\n",
        "        self.plot_cf_matrix(y_true, y_pred, model_pth)\n",
        "        print(progress)\n",
        "\n",
        "    def validate_RUL(self, dl, model_class, model_pth, inp_size):\n",
        "        device = 'cuda:0'\n",
        "        model = torch.jit.load(model_pth)\n",
        "        accs = AverageMeter()\n",
        "        y_pred, y_true, y_scores = [], [], []\n",
        "\n",
        "        # switch to evaluate mode\n",
        "        model.eval()\n",
        "        losses = {\n",
        "              'softmax': AverageMeter(),\n",
        "              }\n",
        "        criterion = {\n",
        "            'softmax': nn.CrossEntropyLoss().to(device)\n",
        "            }\n",
        "        with tqdm(total=int(len(dl.dataset)/self.batch_size)) as pbar:\n",
        "            with torch.no_grad():\n",
        "                for i, (images, target) in enumerate(dl):\n",
        "\n",
        "                    images = images.to(device)\n",
        "                    target = target.to(device)\n",
        "\n",
        "                    # compute output\n",
        "                    output = model(images.to('cuda:0'))\n",
        "                    l_softmax = criterion['softmax'](output, target)\n",
        "                    # measure accuracy and record loss\n",
        "                    acc, pred = accuracy(output, target)\n",
        "                    losses['softmax'].update(l_softmax.item(), images.size(0))\n",
        "                    accs.update(acc.item(), images.size(0))\n",
        "                    # collect for metrics\n",
        "                    y_pred.append(pred)\n",
        "                    y_true.append(target)\n",
        "                    y_scores.append(output.data)\n",
        "            pbar.set_description(\"Processing \")\n",
        "        metrics = calc_metrics(y_pred, y_true, y_scores)\n",
        "        avg_exec_time = self.computeTime(model, inp_size)\n",
        "        progress = (\n",
        "            f'{model_class}-{model_pth}: | '\n",
        "            f'Ls={losses[\"softmax\"].avg:.4f} | '\n",
        "            f'acc={accs.avg:.4f} | '\n",
        "            f'rec={metrics[\"rec\"]:.4f} | '\n",
        "            f'f1={metrics[\"f1\"]:.4f} | '\n",
        "            f'Evaluation time={avg_exec_time:4f} |'\n",
        "        )\n",
        "        self.plot_cf_matrix(y_true, y_pred, model_pth)\n",
        "        print(progress)\n",
        "\n"
      ],
      "metadata": {
        "id": "nTjUxwCrsE-t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jit_location = '/content/jit_models'\n",
        "toval = [( jit_location + '/ResnetRUL_cuda.pth','RUL', 224),\n",
        "         ( jit_location + '/GhostRUL_cuda.pth','RUL', 112),\n",
        "         ( jit_location + '/MobileRUL_cuda.pth','RUL', 112),\n",
        "         ( jit_location + '/originalDACL_cuda.pth', 'DACL', 224),\n",
        "         ( jit_location + '/MobibeDACL_cuda.pth', 'DACL', 112),\n",
        "\n",
        "]                                                   \n",
        "val = validator()\n",
        "for m in toval:\n",
        "  val.validate(*m)"
      ],
      "metadata": {
        "id": "s1xJyisK8yLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further experements"
      ],
      "metadata": {
        "id": "uDpBUQ4V5eja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some code for knowledge distillation experements\n",
        "# !pip install KD_lib\n",
        "\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# from torchvision import datasets, transforms\n",
        "# from KD_Lib.KD import VanillaKD\n",
        "\n",
        "# # Define datasets, dataloaders, models and optimizers\n",
        "# train_dataset = RafDataset(phase='train', transform=test_tf)\n",
        "# test_dataset = RafDataset(phase='test', transform=test_tf)\n",
        "# teacher_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                               # sampler=ImbalancedDatasetSampler(student_dataset),\n",
        "#                                                batch_size = 128,\n",
        "#                                                num_workers = 2,\n",
        "#                                                pin_memory = True)\n",
        "# student_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                               # sampler=ImbalancedDatasetSampler(student_dataset),\n",
        "#                                                batch_size = 128,\n",
        "#                                                num_workers = 2,\n",
        "#                                                pin_memory = True)\n",
        "\n",
        "\n",
        "# teacher_model = # Here shoud be resnet18 DACL model\n",
        "# student_model = # Here sould be resnet9 DACL model\n",
        "\n",
        "# teacher_optimizer = optim.SGD(teacher_model.parameters(), 0.01)\n",
        "# student_optimizer = optim.SGD(student_model.parameters(), 0.01)\n",
        "\n",
        "# # Now, this is where KD_Lib comes into the picture\n",
        "\n",
        "# distiller = VanillaKD(teacher_model, student_model, teacher_loader, student_loader,\n",
        "#                       teacher_optimizer, student_optimizer)\n",
        "# # distiller.train_teacher(epochs=5, plot_losses=True, save_model=True)    # Train the teacher network\n",
        "# distiller.train_student(epochs=5, plot_losses=True, save_model=True)    # Train the student network\n",
        "# distiller.evaluate(teacher=False)                                       # Evaluate the student network\n",
        "# distiller.get_parameters()       "
      ],
      "metadata": {
        "id": "cErHXGwYQQx0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tain Swin Transformer\n",
        "def train_swin(swin, batch_size = 256, \n",
        "                 workers = 2, lr = 0.1, epochs = 40):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.enabled = True\n",
        "\n",
        "    model = swin\n",
        "    model.to(device)\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        \n",
        "        transforms.RandomApply([\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.RandomCrop(224, padding=32)\n",
        "            ], p=0.3),\n",
        "        \n",
        "        transforms.RandomApply([\n",
        "                transforms.ColorJitter(brightness=0.05, contrast=0.05, \n",
        "                                      saturation=0.05, hue=0.05)\n",
        "            ], p=0.3),\n",
        "        transforms.RandomApply([\n",
        "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "            ], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(scale=(0.02,0.125)),\n",
        "        ])\n",
        "\n",
        "    # data_transforms = transforms.Compose([\n",
        "    #     transforms.Resize((224, 224)),\n",
        "    #     transforms.RandomHorizontalFlip(),\n",
        "    #     transforms.RandomApply([\n",
        "    #             transforms.RandomRotation(20),\n",
        "    #             transforms.RandomCrop(224, padding=32)\n",
        "    #         ], p=0.3),\n",
        "    #     transforms.RandomApply([\n",
        "    #             transforms.ColorJitter(brightness=0.05, contrast=0.05, \n",
        "    #                                   saturation=0.05, hue=0.025)\n",
        "    #         ], p=0.3),\n",
        "        \n",
        "    #     transforms.ToTensor(),\n",
        "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "    #                              std=[0.229, 0.224, 0.225]),\n",
        "    #     transforms.RandomErasing(scale=(0.02,0.25)),\n",
        "    #     ])\n",
        "    \n",
        "    train_dataset = RafDataset(phase = 'train', transform = data_transforms)    \n",
        "    \n",
        "    print('Whole train set size:', train_dataset.__len__())\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                              # sampler=ImbalancedDatasetSampler(train_dataset),\n",
        "                                               shuffle = True,  \n",
        "                                               batch_size = batch_size,\n",
        "                                               num_workers = workers,\n",
        "                                               pin_memory = True)\n",
        "\n",
        "    data_transforms_val = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])])   \n",
        "\n",
        "    val_dataset = RafDataset(phase = 'test', transform = data_transforms_val)   \n",
        "\n",
        "    print('Validation set size:', val_dataset.__len__())\n",
        "    \n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                               batch_size = batch_size,\n",
        "                                               num_workers = workers,\n",
        "                                               shuffle = False,  \n",
        "                                               pin_memory = True)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    params = model.parameters()\n",
        "    optimizer = torch.optim.SGD(params,lr=lr, weight_decay = 1e-4, momentum=0.9)\n",
        "    # optimizer = torch.optim.AdamW(params,lr=lr, \n",
        "                                  # weight_decay = 1e-4, amsgrad=True)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        running_loss = 0.0\n",
        "        correct_sum = 0\n",
        "        iter_cnt = 0\n",
        "        model.train()\n",
        "\n",
        "        for (imgs, targets) in train_loader:\n",
        "            iter_cnt += 1\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            out = model(imgs)\n",
        "\n",
        "            loss = criterion(out,targets)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss\n",
        "            _, predicts = torch.max(out, 1)\n",
        "            correct_num = torch.eq(predicts, targets).sum()\n",
        "            correct_sum += correct_num\n",
        "\n",
        "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
        "        running_loss = running_loss/iter_cnt\n",
        "        tqdm.write('[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f' % (epoch, acc, running_loss,optimizer.param_groups[0]['lr']))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.0\n",
        "            iter_cnt = 0\n",
        "            bingo_cnt = 0\n",
        "            sample_cnt = 0\n",
        "            \n",
        "            ## for calculating balanced accuracy\n",
        "            y_true = []\n",
        "            y_pred = []\n",
        "\n",
        "            model.eval()\n",
        "            for (imgs, targets) in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                \n",
        "                out = model(imgs)\n",
        "                loss = criterion(out,targets) \n",
        "                \n",
        "                running_loss += loss\n",
        "                iter_cnt+=1\n",
        "                _, predicts = torch.max(out, 1)\n",
        "                correct_num  = torch.eq(predicts,targets)\n",
        "                bingo_cnt += correct_num.sum().cpu()\n",
        "                sample_cnt += out.size(0)\n",
        "                \n",
        "                y_true.append(targets.cpu().numpy())\n",
        "                y_pred.append(predicts.cpu().numpy())\n",
        "        \n",
        "            running_loss = running_loss/iter_cnt   \n",
        "            scheduler.step()\n",
        "\n",
        "            acc = bingo_cnt.float()/float(sample_cnt)\n",
        "            acc = np.around(acc.numpy(),4)\n",
        "            best_acc = max(acc,best_acc)\n",
        "\n",
        "            y_true = np.concatenate(y_true)\n",
        "            y_pred = np.concatenate(y_pred)\n",
        "            balanced_acc = np.around(balanced_accuracy_score(y_true, y_pred),4)\n",
        "\n",
        "            tqdm.write(\"[Epoch %d] Validation accuracy:%.4f. bacc:%.4f. Loss:%.3f\" % (epoch, acc, balanced_acc, running_loss))\n",
        "            tqdm.write(\"best_acc:\" + str(best_acc))\n",
        "\n",
        "            if acc > 0.86 and acc == best_acc:\n",
        "                torch.save({'iter': epoch,\n",
        "                            'model_state_dict': model.state_dict(),\n",
        "                             'optimizer_state_dict': optimizer.state_dict(),},\n",
        "                            os.path.join('/home/mangaboba/environ/fer/checkpoints', \"rafdb_epoch\"+'_swin'+str(epoch)+\"_acc\"+str(acc)+\"_bacc\"+str(balanced_acc)+\".pth\"))\n",
        "                tqdm.write('Model saved.')\n"
      ],
      "metadata": {
        "id": "nTEKCfCRbwrR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}